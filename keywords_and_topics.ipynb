{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Setup (must run cells marked *)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "`pip install beautifulsoup4 bertopic keybert keyphrase_vectorizers libpff-python scikit-learn`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Imports*\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import random\n",
            "import textwrap\n",
            "\n",
            "import pandas as pd"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Get emails from PST file(s)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pypff\n",
            "\n",
            "def get_messages_from_pst(pst_path, output_folder):\n",
            "    if not os.path.isdir(output_folder):\n",
            "        os.makedirs(output_folder, exist_ok=True)\n",
            "    file_ = pypff.open(pst_path)\n",
            "    root = file_.get_root_folder()\n",
            "    for x in root.sub_items:\n",
            "        walk_folder_for_messages(x, output_folder=output_folder)\n",
            "\n",
            "\n",
            "def walk_folder_for_messages(folder, output_folder):\n",
            "    for i in folder.sub_items:\n",
            "        if type(i) == pypff.message:\n",
            "            subject = i.subject\n",
            "            text = i.plain_text_body.decode()\n",
            "            message = f'{subject}\\n{text}'\n",
            "            with open(os.path.join(output_folder, str(i.identifier)), 'w', encoding='utf-8') as f:\n",
            "                f.write(message)\n",
            "        elif type(i) == pypff.folder:\n",
            "            walk_folder_for_messages(i, output_folder=output_folder)\n",
            "\n",
            "pst_path_list = ['sample.pst', 'test.pst'] # Specify paths\n",
            "output_folder = 'email_text'\n",
            "for email_path in pst_path_list:\n",
            "    get_messages_from_pst(email_path, output_folder)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### ... Or from files with full headers, etc.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from bs4 import BeautifulSoup\n",
            "\n",
            "def get_email_text_and_subject(path, combine=True, stripHTML=True):\n",
            "    lines = open(path, encoding='utf-8').read().split('\\n')\n",
            "    start_index, end_index = None, None\n",
            "    subject = None\n",
            "    for index, line in enumerate(lines):\n",
            "        if line.startswith('Subject:') and subject is None:\n",
            "            subject = line.split(':', 1)[1].strip()\n",
            "        if not line and start_index is None:\n",
            "            start_index = index\n",
            "        elif '-----Original Message-----' in line:\n",
            "            end_index = index\n",
            "            break\n",
            "\n",
            "    text = '\\n'.join(lines[start_index:end_index]).strip()\n",
            "    if stripHTML:\n",
            "        text = BeautifulSoup(text, 'html.parser').text\n",
            "    if combine:\n",
            "        return f'{subject}\\n{text}'\n",
            "    return {'main_text': text, 'subject': subject}\n",
            "\n",
            "source_folder = None # Specify source folder\n",
            "output_folder = 'email_text'\n",
            "for root, dirs, filenames in os.walk(source_folder):\n",
            "    for filename in filenames:\n",
            "        email = get_email_text_and_subject(open(os.path.join(root, filename), encoding='utf-8'))\n",
            "        with open(os.path.join(output_folder, filename), 'w', encoding='utf-8') as f:\n",
            "            f.write(email)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Specify folder contaning outputted email text, one email per file*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "email_folder = 'email_text'"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Keyword Extraction"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Setup\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from keybert import KeyBERT\n",
            "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "\n",
            "kw_model = KeyBERT()\n",
            "\n",
            "def get_keywords(text, kwargs):\n",
            "    if kwargs.get('vectorizer'):\n",
            "        if kwargs['vectorizer'] == 'keyphrase':\n",
            "            kwargs['vectorizer'] = KeyphraseCountVectorizer()\n",
            "        else:\n",
            "            kwargs['vectorizer'] = CountVectorizer(\n",
            "                ngram_range=kwargs.get('keyphrase_ngram_range', (1, 1)),\n",
            "                stop_words=kwargs['stop_words']\n",
            "            )\n",
            "    keywords = kw_model.extract_keywords(text, **kwargs)\n",
            "    keywords = [x[0] for x in keywords]\n",
            "    print('KEYWORDS:\\n')\n",
            "    print('\\n'.join(keywords))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Extract keywords from a random or specified email (see KWARGS)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "email_path = os.path.join(email_folder, (random.choice(os.listdir(email_folder))))\n",
            "email = open(email_path, encoding='utf-8').read()\n",
            "\n",
            "KWARGS = {\n",
            "    'keyphrase_ngram_range': (1, 3), # Min, max word count for keywords\n",
            "    'use_mmr': True, # Increases diversity of keywords\n",
            "    'diversity': .5, # Set diversity between 0 and 1 if using MMR\n",
            "    'vectorizer': 'keyphrase', # (\"keyphrase\", True, False) How to represent document. Keyphrase vectorizer should be more coherent\n",
            "    'stop_words': 'english'\n",
            "}\n",
            "\n",
            "print(f'FILENAME: {email_path}\\n')\n",
            "print('TEXT:\\n ', \"\\n\".join(textwrap.wrap(email, 100)), '\\n')\n",
            "get_keywords(email, KWARGS)\n",
            "\n",
            "# Run on same email with different args\n",
            "# print()\n",
            "# KWARGS['vectorizer'] = True\n",
            "# get_keywords(email, KWARGS)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Topic Modeling"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Setup"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from bertopic import BERTopic\n",
            "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
            "from bertopic.vectorizers import ClassTfidfTransformer"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Get topics and view topic hierarchy (see comments)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Select subset or all emails in email folder\n",
            "slice_docs = (None, 1000)\n",
            "docs = [open(os.path.join(email_folder, filename), encoding='utf-8').read() for \n",
            "        filename in os.listdir(email_folder)[slice_docs[0]:slice_docs[1]]]\n",
            "\n",
            "# Document representations to chain and feed into topic model\n",
            "representations = [\n",
            "    KeyBERTInspired(), # Should make topics more coherent\n",
            "    MaximalMarginalRelevance(diversity=0.3), # Makes topics more diverse\n",
            "]\n",
            "\n",
            "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
            "\n",
            "topic_model = BERTopic(\n",
            "        representation_model=representations, # Representations (see above)\n",
            "        ctfidf_model=ctfidf_model,  # Prevents very frequent words in data from being candidate topics\n",
            "        nr_topics='auto' # Topic reduction. Set number of desired topics, 'auto' for auto-reduction, \n",
            "        # or None. Set to None if there aren't enough topic. \n",
            "        )\n",
            "topics, probabilities = topic_model.fit_transform(docs)\n",
            "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
            "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### View topics as table"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "topic_model.get_topic_info()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Show docs per topic(s)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Select topic(s) by int\n",
            "topics_to_show = [0, 1]\n",
            "# Show first n docs\n",
            "n_docs_to_show = 10\n",
            "# Show first n characters of each doc\n",
            "n_chars_per_doc = 500\n",
            "\n",
            "df = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
            "df = df.loc[df['Topic'].isin(topics_to_show)].head(n_docs_to_show)\n",
            "docs_ = df['Document'].to_list()\n",
            "random.shuffle(docs_)\n",
            "for i, doc in enumerate(docs_):\n",
            "    print(f'DOC {i + 1}\\n----------\\n')\n",
            "    print('\\n'.join(textwrap.wrap(doc[:n_chars_per_doc] + ' ...' if n_chars_per_doc else '')), '\\n')"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "env",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.3"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
