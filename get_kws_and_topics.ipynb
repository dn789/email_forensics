{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "\n",
            "### Keyword search and topic modelling"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Dependencies\n",
            "\n",
            "- `pip install beautifulsoup4 bertopic keybert keyphrase_vectorizers scikit-learn`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Select email folder\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "email_folder = 'data/extracted emails/process_pst_js/alliancecoal/'\n",
            "\n",
            "#################################\n",
            "\n",
            "import os\n",
            "email_paths = []\n",
            "for root, dirs, files in os.walk(email_folder):\n",
            "    for file in files:\n",
            "        email_paths.append(os.path.join(root, file))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Keyword Extraction\n",
            "- Setup"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "import random\n",
            "import textwrap\n",
            "\n",
            "from keybert import KeyBERT\n",
            "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "\n",
            "kw_model = KeyBERT()\n",
            "\n",
            "def get_keywords(text, kwargs):\n",
            "    if kwargs.get('vectorizer'):\n",
            "        if kwargs['vectorizer'] == 'keyphrase':\n",
            "            kwargs['vectorizer'] = KeyphraseCountVectorizer()\n",
            "        else:\n",
            "            kwargs['vectorizer'] = CountVectorizer(\n",
            "                ngram_range=kwargs.get('keyphrase_ngram_range', (1, 1)),\n",
            "                stop_words=kwargs['stop_words']\n",
            "            )\n",
            "    keywords = kw_model.extract_keywords(text, **kwargs)\n",
            "    keywords = [x[0] for x in keywords]\n",
            "    print('KEYWORDS:\\n')\n",
            "    print('\\n'.join(keywords))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "- Extract keywords from a random or specified email (see KWARGS)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "KWARGS = {\n",
            "    'keyphrase_ngram_range': (1, 3), # Min, max word count for keywords\n",
            "    'use_mmr': True, # Increases diversity of keywords\n",
            "    'diversity': .5, # Set diversity between 0 and 1 if using MMR\n",
            "    'vectorizer': 'keyphrase', # (\"keyphrase\", True, False) How to represent document. Keyphrase vectorizer should be more coherent\n",
            "    'stop_words': 'english'\n",
            "}\n",
            "\n",
            "#################################\n",
            "text = None\n",
            "\n",
            "while not text:\n",
            "    email_path = random.choice((email_paths))\n",
            "    text = json.load(open(email_path, encoding='utf-8')).get('bodyText', '').strip()\n",
            "\n",
            "\n",
            "print(f'FILENAME: {email_path}\\n')\n",
            "print('TEXT:\\n ', \"\\n\".join(textwrap.wrap(text, 100)), '\\n')\n",
            "get_keywords(text, KWARGS)\n",
            "\n",
            "# Run on same email with different args\n",
            "# print()\n",
            "# KWARGS['vectorizer'] = True\n",
            "# get_keywords(email, KWARGS)\n",
            "\n",
            "text = None"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Topic Modeling"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "- Imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "import random\n",
            "import textwrap\n",
            "\n",
            "from bertopic import BERTopic\n",
            "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
            "from bertopic.vectorizers import ClassTfidfTransformer"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "- Get topics and view topic hierarchy (see comments)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Select subset or all emails in email folder\n",
            "slice_docs = (None, None)\n",
            "\n",
            "# Document representations to chain and feed into topic model\n",
            "representations = [\n",
            "    KeyBERTInspired(), # Should make topics more coherent\n",
            "    MaximalMarginalRelevance(diversity=0.3), # Makes topics more diverse\n",
            "]\n",
            "\n",
            "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
            "\n",
            "topic_model = BERTopic(\n",
            "        representation_model=representations, # Representations (see above)\n",
            "        ctfidf_model=ctfidf_model,  # Prevents very frequent words in data from being candidate topics\n",
            "        nr_topics='auto' # Topic reduction. Set number of desired topics, 'auto' for auto-reduction, \n",
            "        # or None. Set to None if there aren't enough topic. \n",
            "        )\n",
            "\n",
            "#################################\n",
            "\n",
            "docs = [json.load(open(path, encoding='utf-8')).get('bodyText', '') for \n",
            "        path in email_paths[slice_docs[0]:slice_docs[1]]]\n",
            "docs = [doc for doc in docs if doc.strip()]\n",
            "\n",
            "topics, probabilities = topic_model.fit_transform(docs)\n",
            "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
            "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "- View topics as table"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "topic_model.get_topic_info()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "- Show docs per topic(s)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Select topic(s) by number (must be list). Get topic numbers from table or \n",
            "# labels in topic tree , ie. \"1_trees_grass_nature\" topic number is 1. \n",
            "topics_to_show = [0]\n",
            "# Show first n docs\n",
            "n_docs_to_show = 10\n",
            "# Show first n characters of each doc\n",
            "n_chars_per_doc = 500\n",
            "\n",
            "#################################\n",
            "\n",
            "df = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
            "df = df.loc[df['Topic'].isin(topics_to_show)].head(n_docs_to_show)\n",
            "docs_ = df['Document'].to_list()\n",
            "random.shuffle(docs_)\n",
            "for i, doc in enumerate(docs_):\n",
            "    print(f'DOC {i + 1}\\n----------\\n')\n",
            "    print('\\n'.join(textwrap.wrap(doc[:n_chars_per_doc] + ' ...' if n_chars_per_doc else '')), '\\n')"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "env",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.18"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
